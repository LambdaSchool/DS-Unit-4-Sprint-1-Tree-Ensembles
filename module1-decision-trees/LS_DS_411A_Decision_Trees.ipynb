{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_411A_Decision_Trees.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "A0-sUApS7MIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_Lambda School Data Science — Tree Ensembles_ \n",
        "\n",
        "# Decision Trees\n",
        "\n",
        "### Pre-read / pre-watch\n",
        "- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
        "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) — _Don’t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
        "- [How a Russian mathematician constructed a decision tree - by hand - to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n"
      ]
    },
    {
      "metadata": {
        "id": "cRr2wuYV8DmA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Categorical encoding for trees\n",
        "\n",
        "#### Good reads\n",
        "- [Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)\n",
        "- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n",
        "- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n",
        "- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
        "\n",
        "#### Some options\n",
        "- [category_encoders.target_encoder.OrdinalEncoder](http://contrib.scikit-learn.org/categorical-encoding/ordinal.html)\n",
        "- [category_encoders.target_encoder.TargetEncoder](http://contrib.scikit-learn.org/categorical-encoding/targetencoder.html)\n",
        "- [sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) + [sklearn.compose.ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)\n",
        "- [sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) + [sklearn-pandas](https://github.com/scikit-learn-contrib/sklearn-pandas)\n",
        "- [kaggler.preprocessing.data.OneHotEncoder(min_obs=...)](https://pythonhosted.org/Kaggler/kaggler.preprocessing.html#kaggler.preprocessing.data.OneHotEncoder)\n",
        "- [Create your own scikit-learn compatible transformers!](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Python-and-Scikit-Learn/blob/master/code/Feature%20Engineering.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "xQ_JHVbmzLeT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Libraries\n",
        "\n",
        "This notebook optionally uses the [dtreeviz](https://explained.ai/decision-tree-viz/) library for decision tree visualizations.\n",
        "\n",
        "It is easy to install on Google Colab: `!pip install dtreeviz`\n",
        "\n",
        "But the [installation instructions](https://github.com/parrt/dtreeviz#install) are harder on Mac or Windows. \n",
        "\n",
        "So you have several options with this notebook:\n",
        "1. Run on Google Colab\n",
        "2. Install dtreeviz on your system\n",
        "3. Skip the cells with dtreeviz"
      ]
    },
    {
      "metadata": {
        "id": "gB1mD5QH0Xz9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install dtreeviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HUMXG9hW0za9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook also uses the [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding) library.\n",
        "\n",
        "To install on Google Colab: `!pip install category_encoders`\n",
        "\n",
        "To install with Anaconda: `!conda install -c conda-forge category_encoders`\n"
      ]
    },
    {
      "metadata": {
        "id": "zn6-U2BS0y7T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWm7mMlH9sl2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Play Tennis"
      ]
    },
    {
      "metadata": {
        "id": "2f5-S87kg6gJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll reproduce the \"Play Tennis\" example from Ross Quinlan's 1986 paper, [Induction of Decison Trees](https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf).\n",
        "\n",
        "[According to Wikipedia](https://en.wikipedia.org/wiki/Ross_Quinlan), \"John Ross Quinlan is a computer science researcher in data mining and decision theory. He has contributed extensively to the development of decision tree algorithms, including inventing the canonical C4.5 and ID3 algorithms.\""
      ]
    },
    {
      "metadata": {
        "id": "Qfv6Zwdoje7_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\"Table 1 shows a small training set\"***"
      ]
    },
    {
      "metadata": {
        "id": "oUE-G0pgg58u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = 'No. Outlook Temperature Humidity Windy Class'.split()\n",
        "\n",
        "raw = \"\"\"1 sunny hot high false N\n",
        "2 sunny hot high true N\n",
        "3 overcast hot high false P\n",
        "4 rain mild high false P\n",
        "5 rain cool normal false P\n",
        "6 rain cool normal true N\n",
        "7 overcast cool normal true P\n",
        "8 sunny mild high false N\n",
        "9 sunny cool normal false P\n",
        "10 rain mild normal false P\n",
        "11 sunny mild normal true P\n",
        "12 overcast mild high true P\n",
        "13 overcast hot normal false P\n",
        "14 rain mild high true N\"\"\"\n",
        "\n",
        "data = [row.split() for row in raw.split('\\n')]\n",
        "\n",
        "tennis = pd.DataFrame(data, columns=columns).set_index('No.')\n",
        "\n",
        "tennis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvFu9kvJj9kk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\"A decision tree that correctly classifies each object in the training set is given in Figure 2.\"***\n",
        "\n",
        "<img src=\"https://i.imgur.com/RD7d0u0.png\" height=\"300\">"
      ]
    },
    {
      "metadata": {
        "id": "dT0ghLwd-Xzp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explore the data\n",
        "(and compare to Figure 2 above)"
      ]
    },
    {
      "metadata": {
        "id": "kHkkeALqjNiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this dataset, the tennis player decided to play on 9 days, and decided not to on 5 days."
      ]
    },
    {
      "metadata": {
        "id": "qeLDinBihMDQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tennis['Class'] = (tennis['Class'] == 'P').astype(int)\n",
        "tennis['Class'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ype2-apnlaG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The tennis player played on 4 overcast days, 3 rainy days, and 2 sunny days."
      ]
    },
    {
      "metadata": {
        "id": "LfBMB0Soh58T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tennis.groupby('Outlook')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jx3-MFvalrQC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On sunny days, the tennis player's decision depends on the humidity. (The Outlook and Humidity features interact.)"
      ]
    },
    {
      "metadata": {
        "id": "FmVN7hwEiHE5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tennis[tennis['Outlook']=='sunny'].groupby('Humidity')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0it0HsFl47M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On rainy days, the tennis player's decision depends on the wind. (The Outlook and Windy features interact.)"
      ]
    },
    {
      "metadata": {
        "id": "CeOd7rGeivJp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tennis[tennis['Outlook']=='rain'].groupby('Windy')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UvOkHk8o-h_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train a Decision Tree\n",
        "The tree has 100% accuracy"
      ]
    },
    {
      "metadata": {
        "id": "VCyn34nEi0nz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X = pd.get_dummies(tennis.drop(columns='Class'))\n",
        "y = tennis['Class']\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X, y)\n",
        "dt.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwF0phe6-oR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the tree\n",
        "With dtreeviz:"
      ]
    },
    {
      "metadata": {
        "id": "86CGfYRFxRid",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from dtreeviz.trees import dtreeviz\n",
        "\n",
        "dtreeviz(dt, X, y, \n",
        "         target_name='Play Tennis',\n",
        "         feature_names=X.columns, \n",
        "         class_names=['No', 'Yes'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4vTK0G2-rkZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With scikit-learn:"
      ]
    },
    {
      "metadata": {
        "id": "pzJSj7zbnVaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "dot_data = export_graphviz(dt, out_file=None, \n",
        "                           feature_names=X.columns, \n",
        "                           class_names=['No', 'Play Tennis'],   \n",
        "                           filled=True, impurity=False, rounded=True)\n",
        "graphviz.Source(dot_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uNPrbfS-vzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compare to Logistic Regression\n",
        "Logistic Regression has lower accuracy here:"
      ]
    },
    {
      "metadata": {
        "id": "67JrwiOn1S5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(solver='lbfgs')\n",
        "lr.fit(X, y)\n",
        "lr.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jsHqsVZ-3NB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression model has these coefficients:"
      ]
    },
    {
      "metadata": {
        "id": "bab2XOFU1cT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.Series(lr.coef_[0], X.columns).sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SMkfr8Ic_Q9z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What if we use \"Ordinal\" encoding instead of One-Hot?\n",
        "\n",
        "The \"ordinal\" encoding is applied arbitrarily here.\n",
        "\n",
        "The decision tree still has 100% accuracy."
      ]
    },
    {
      "metadata": {
        "id": "Uk9qetb-pnq-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "\n",
        "encoder = ce.OrdinalEncoder()\n",
        "X = encoder.fit_transform(tennis.drop(columns='Class'))\n",
        "y = tennis['Class']\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X, y)\n",
        "dt.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BX4TPGMz_f98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A tree's [feature importances](https://explained.ai/rf-importance/index.html#3) are non-negative. They have magnitude but no direction. \n",
        "\n",
        "\"Outlook\" is the most important feature in this model."
      ]
    },
    {
      "metadata": {
        "id": "XYKDrONX3LeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.Series(dt.feature_importances_, X.columns).sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1gHapIbXAuyq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With the arbitrary \"ordinal\" encoding, the Logistic Regression accuracy decreases to 64% (compared to 86% with One-Hot Encoding)."
      ]
    },
    {
      "metadata": {
        "id": "5yaEpKT_pw-U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(solver='lbfgs')\n",
        "lr.fit(X, y)\n",
        "lr.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2xzV_JIA6HS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Why? Because now the relatioship between some features and target are not monotonic. \n",
        "\n",
        "We can look this, one feature at a time."
      ]
    },
    {
      "metadata": {
        "id": "KJ6nAkKH2PjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xy = pd.concat([X, y], axis='columns')\n",
        "Xy.groupby('Outlook')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YoM760bA2jfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xy.groupby('Temperature')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L-LuZt1q2oHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xy.groupby('Humidity')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tmlt8FOc2suk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xy.groupby('Windy')['Class'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pg-8a9zJBNC4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The \"Humidity\" and \"Windy\" features have monotnic relationships with the target. The other two features do not. This is shown in the model's coefficients.\n",
        "\n",
        "We know that the \"Outlook\" feature is important in this dataset, but Logistic Regression cannot model it with an arbitrary \"ordinal\" encoding."
      ]
    },
    {
      "metadata": {
        "id": "bFKdxth92w8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.Series(lr.coef_[0], X.columns).sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6xcjCw5LDeYk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See the links at the top of this notebook to learn more about categorical encoding for trees."
      ]
    },
    {
      "metadata": {
        "id": "opIsHEofDobK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shallow trees are good for fast, first baselines, and to look for \"leakage\""
      ]
    },
    {
      "metadata": {
        "id": "EmhDEvqsC2Ln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Xavier Amatriain recommends,](https://www.quora.com/What-are-some-best-practices-for-training-machine-learning-models/answer/Xavier-Amatriain)\n",
        "\n",
        "\"Make sure your training features do not contain data from the “future” (aka time traveling). While this might be easy and obvious in some cases, it can get tricky. ... If your test metric becomes really good all of the sudden, ask yourself what you might be doing wrong. Chances are you are time travelling or overfitting in some way.\""
      ]
    },
    {
      "metadata": {
        "id": "7SFZ8N07EAeZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can test this with the [UCI repository's Bank Marketing dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing). It has a feature which leaks information from the future and should be dropped:\n",
        "\n",
        "\"11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input ... should be discarded if the intention is to have a realistic predictive model.\"\n",
        "\n",
        "So let's download the data ..."
      ]
    },
    {
      "metadata": {
        "id": "VOI04zrQ4o27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
        "!unzip bank-additional.zip\n",
        "%cd bank-additional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UcRFbDQlEtkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "... and train a shallow tree baseline, without dropping the `duration` feature."
      ]
    },
    {
      "metadata": {
        "id": "_aUpZCdK3sDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "df = pd.read_csv('bank-additional-full.csv', sep=';')\n",
        "X = df.drop(columns='y')\n",
        "y = df['y'] == 'yes'\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "pipe = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    DecisionTreeClassifier(max_depth=2)\n",
        ")\n",
        "\n",
        "cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rx1QAeKcE0KK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This baseline has a ROC AUC score above 0.85, and it uses the `duration` feature, as well as `nr.employed`, a \"social and economic context attribute\" for \"number of employees - quarterly indicator.\""
      ]
    },
    {
      "metadata": {
        "id": "O0k5m1t-4FC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train, y_train)\n",
        "tree = pipe.named_steps['decisiontreeclassifier']\n",
        "encoder = pipe.named_steps['onehotencoder']\n",
        "feature_names = encoder.transform(X_train).columns\n",
        "feature_names[tree.feature_importances_ > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f8mftWTIFQFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can visualize the tree:"
      ]
    },
    {
      "metadata": {
        "id": "eiTVyfZ54Ue7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from dtreeviz.trees import dtreeviz\n",
        "\n",
        "class_names = ['No', 'Yes']\n",
        "dtreeviz(tree, encoder.transform(X_train), y_train, \n",
        "         target_name='Subscribed',\n",
        "         feature_names=list(feature_names), \n",
        "         class_names=class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipE3XsY74M41",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "dot_data = export_graphviz(tree, out_file=None, \n",
        "                           feature_names=feature_names, class_names=class_names, \n",
        "                           filled=True, impurity=False, proportion=True)\n",
        "\n",
        "graphviz.Source(dot_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcSQKvuLGzFj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### When the `duration` feature is dropped, \n",
        "then the ROC AUC score drops from ~0.85 to ~0.75"
      ]
    },
    {
      "metadata": {
        "id": "_pNEgSeE6E3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.drop(columns='duration')\n",
        "cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96nxtRRHHOYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ROC AUC\n",
        "\n",
        "[Wikipedia explains,](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) \"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\"\n",
        "\n",
        "ROC AUC is the area under the ROC curve. [It can be interpreted](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it) as \"the expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\" \n",
        "\n",
        "ROC AUC measures how well a classifier ranks predicted probabilities. It ranges from 0 to 1. A naive majority class baseline will have an ROC AUC score of 0.5. "
      ]
    },
    {
      "metadata": {
        "id": "I6UAMxPjFkq3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_pred_proba = cross_val_predict(pipe, X_train, y_train, cv=5, n_jobs=-1, \n",
        "                                 method='predict_proba')[:, 1]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_pred_proba)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "print('Area under the Receiver Operating Characteristic curve:', \n",
        "      roc_auc_score(y_train, y_pred_proba))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HXFjtW39GJHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'False Positive Rate': fpr, \n",
        "              'True Positive Rate': tpr, \n",
        "              'Threshold': thresholds})"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}