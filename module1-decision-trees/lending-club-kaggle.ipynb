{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv('test_features.csv')\n",
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_labels = pd.read_csv('train_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.describe()\n",
    "# train_features.shape  # 37745, 103\n",
    "# train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From kaggle, already have data split out into train and test, so can assign directly\n",
    "# Credit to DMA/LSDS01\n",
    "\n",
    "X_train = train_features.dropna(axis='columns', how='any')\n",
    "# X_train = train_features.drop(columns=['id', 'member_id'])\n",
    "y_train = train_labels.charged_off  # labels are 'id' and 'charged_off'\n",
    "\n",
    "X_test = test_features.dropna(axis='columns', how='any')\n",
    "# X_test = test_features.drop(columns=['pct_tl_nvr_dlq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37745, 66), (9437, 67), (37745,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct_tl_nvr_dlq in X_test but not in X_train\n"
     ]
    }
   ],
   "source": [
    "for col in X_test.columns:\n",
    "    if col not in X_train.columns:\n",
    "        print(col, 'in X_test but not in X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns=['pct_tl_nvr_dlq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model a Decision Tree regression - HT RH/LSDS\n",
    "\n",
    "def regress_wave(max_depth):\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    print('Train R^2 score:', tree.score(X_train, y_train))\n",
    "    # print('Test R^2 score:', tree.score(X_test, y_test))\n",
    "    plt.scatter(X_train, y_train)\n",
    "    # plt.scatter(X_test, y_test)\n",
    "    plt.step(X, tree.predict(X))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# regress_wave((1,8,1))  # crashes on 'term' feature, which is an object dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw from LSDS01 Unit 2 Tanzania waterpump project methods\n",
    "\n",
    "X_train_no_object_dtypes = X_train.select_dtypes(include='number')\n",
    "# X_train_no_object_dtypes = X_train_no_object_dtypes.dropna()\n",
    "\n",
    "X_test_no_object_dtypes = X_test.select_dtypes(include='number')\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "# model.get_params().keys()\n",
    "\n",
    "param_grid = {}\n",
    "# param_grid = {'decisiontreeclassifier__max_depth': [1, 2, 3, 4, 5, 6, 7, 8]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                           scoring='roc_auc', cv=3, n_jobs=2)  # can define specific args here\n",
    "\n",
    "grid_search.fit(X_train_no_object_dtypes, y_train)\n",
    "# estimator is your model or pipeline, which you've instantiated and fitted\n",
    "\n",
    "# X_test is your dataframe or numpy array, \n",
    "# with the same number of rows, in the same order, as test_features, \n",
    "# and the same number of columns, in the same order, as X_train\n",
    "\n",
    "# y_pred = grid_search.predict(X_test_no_object_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "\n",
    "X_train.isnull().sum().sum(), X_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best score: {grid_search.best_score_:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit DT Classifier predictive probabilities to kaggle\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission['charged_off'] = grid_search.predict_proba(X_test_no_object_dtypes)[:, 1]\n",
    "submission.to_csv('submission-001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try a 'first, fast' Random Forest/OOB model for roc auc on same kaggle/Lending Club data\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline  # sklearn appears to encourage use of pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pipeline\n",
    "\n",
    "pipe = make_pipeline(ce.OrdinalEncoder(),\n",
    "                     RandomForestClassifier(n_estimators=100,\n",
    "                                            class_weight='balanced',\n",
    "                                            min_samples_leaf=0.005,\n",
    "                                            oob_score=True,\n",
    "                                            n_jobs=-1))\n",
    "\n",
    "# Optional use of cross_val_score\n",
    "\n",
    "# cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional feature engineering\n",
    "# \"['member_id' 'url' 'desc'] not found in axis\"\n",
    "\n",
    "# X_train_minus_drops = X_train.drop(columns='id') -- did not significantly increase pred acc\n",
    "# X_test_minus_drops = X_test.drop(columns='id')\n",
    "\n",
    "# Try converting interest rate percentages from strings to floats\n",
    "X_train['int_rate'] = X_train['int_rate'].str.strip('%').astype(float)\n",
    "X_test['int_rate'] = X_test['int_rate'].str.strip('%').astype(float)\n",
    "\n",
    "# Try dropping features with high cardinality\n",
    "# X_train = X_train.drop(columns='zip_code')\n",
    "# X_test = X_test.drop(columns='zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "ROC AUC, Out-of-Bag estimate: 0.7268300409442843\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline, and compute predictive probabilities - all HT RH/LSDS\n",
    "\n",
    "%time\n",
    "pipe.fit(X_train, y_train)\n",
    "# pipe.fit(X_train_minus_drops, y_train)\n",
    "y_pred_proba = pipe.named_steps['randomforestclassifier'].oob_decision_function_[:, 1]\n",
    "print('ROC AUC, Out-of-Bag estimate:', roc_auc_score(y_train, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit RF/OOB predictive probabilities to kaggle\n",
    "\n",
    "submission_02 = sample_submission.copy()\n",
    "submission_02['charged_off'] = pipe.predict_proba(X_test)[:, 1]\n",
    "submission_02.to_csv('submission-002.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New - return to original train and test data to test a different workflow on RF\n",
    "\n",
    "X_train = train_features\n",
    "# X_train = train_features.drop(columns=['id', 'member_id'])\n",
    "y_train = train_labels.charged_off  # labels are 'id' and 'charged_off'\n",
    "\n",
    "X_test = test_features\n",
    "# X_test = test_features.dropna(axis='columns', how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37745, 103), (37745,), (9437, 103))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37745, 98), (9437, 98))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key difference in this workflow is that won't drop nulls immediately,\n",
    "# but will use 'wrange' function from RH/LSDS\n",
    "\n",
    "\n",
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Drop some columns\n",
    "    X = X.drop(columns='id')  # id is random\n",
    "    X = X.drop(columns=['member_id', 'url', 'desc'])  # All null\n",
    "    X = X.drop(columns='title')  # Duplicative of purpose\n",
    "    X = X.drop(columns='grade')  # Duplicative of sub_grade\n",
    "    \n",
    "    # Transform sub_grade from \"A1\" - \"G5\" to 1.1 - 7.5\n",
    "    def wrangle_sub_grade(x):\n",
    "        first_digit = ord(x[0]) - 64\n",
    "        second_digit = int(x[1])\n",
    "        return first_digit + second_digit/10\n",
    "    \n",
    "    X['sub_grade'] = X['sub_grade'].apply(wrangle_sub_grade)\n",
    "\n",
    "    # Convert percentages from strings to floats\n",
    "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
    "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
    "        \n",
    "    # Transform earliest_cr_line to an integer: how many days it's been open\n",
    "    X['earliest_cr_line'] = pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True)\n",
    "    X['earliest_cr_line'] = pd.Timestamp.today() - X['earliest_cr_line']\n",
    "    X['earliest_cr_line'] = X['earliest_cr_line'].dt.days\n",
    "    \n",
    "    # Create features for three employee titles: teacher, manager, owner\n",
    "    X['emp_title'] = X['emp_title'].str.lower()\n",
    "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
    "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
    "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
    "    \n",
    "    # Drop categoricals with high cardinality\n",
    "    X = X.drop(columns=['emp_title', 'zip_code'])\n",
    "    \n",
    "    # Transform features with many nulls to binary flags\n",
    "    many_nulls = ['sec_app_mths_since_last_major_derog',\n",
    "                  'sec_app_revol_util',\n",
    "                  'sec_app_earliest_cr_line',\n",
    "                  'sec_app_mort_acc',\n",
    "                  'dti_joint',\n",
    "                  'sec_app_collections_12_mths_ex_med',\n",
    "                  'sec_app_chargeoff_within_12_mths',\n",
    "                  'sec_app_num_rev_accts',\n",
    "                  'sec_app_open_act_il',\n",
    "                  'sec_app_open_acc',\n",
    "                  'revol_bal_joint',\n",
    "                  'annual_inc_joint',\n",
    "                  'sec_app_inq_last_6mths',\n",
    "                  'mths_since_last_record',\n",
    "                  'mths_since_recent_bc_dlq',\n",
    "                  'mths_since_last_major_derog',\n",
    "                  'mths_since_recent_revol_delinq',\n",
    "                  'mths_since_last_delinq',\n",
    "                  'il_util',\n",
    "                  'emp_length',\n",
    "                  'mths_since_recent_inq',\n",
    "                  'mo_sin_old_il_acct',\n",
    "                  'mths_since_rcnt_il',\n",
    "                  'num_tl_120dpd_2m',\n",
    "                  'bc_util',\n",
    "                  'percent_bc_gt_75',\n",
    "                  'bc_open_to_buy',\n",
    "                  'mths_since_recent_bc']\n",
    "\n",
    "    for col in many_nulls:\n",
    "        X[col] = X[col].isnull()\n",
    "    \n",
    "    # For features with few nulls, do mean imputation\n",
    "    for col in X:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    # Return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "\n",
    "X_train = wrangle(X_train)\n",
    "X_test  = wrangle(X_test)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline [see key previous workflow several lines above]\n",
    "\n",
    "%time\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred_proba = pipe.named_steps['randomforestclassifier'].oob_decision_function_[:, 1]\n",
    "print('ROC AUC, Out-of-Bag estimate:', roc_auc_score(y_train, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit these RF/OOB predictive probabilities to kaggle\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission_03 = sample_submission.copy()\n",
    "submission_03['charged_off'] = pipe.predict_proba(X_test)[:, 1]\n",
    "submission_03.to_csv('submission-003.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-477fa34615c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
